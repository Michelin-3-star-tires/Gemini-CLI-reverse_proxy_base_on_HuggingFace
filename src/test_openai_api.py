#%%
"""
OpenAIå…¼å®¹æ¥å£æµ‹è¯•è„šæœ¬

æ­¤è„šæœ¬ç”¨äºæµ‹è¯• geminicli2api çš„ OpenAI å…¼å®¹æ¥å£
"""
import os

# å¼ºåˆ¶è®¾ç½®ç»ˆç«¯ç¼–ç ä¸ºUTF-8
os.system('chcp 65001 > nul')

#%%
import logging
from openai import OpenAI

# --- æ ‡å‡†åŒ–è·¯å¾„ç®¡ç† ---
current_script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_script_dir)

LOGS_DIR = os.path.join(project_root, 'logs')
os.makedirs(LOGS_DIR, exist_ok=True)

# --- é…ç½®æ—¥å¿— ---
log_file = os.path.join(LOGS_DIR, 'test_openai_api.log')
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logging.captureWarnings(True)

#%%
# --- æµ‹è¯•å‚æ•°é…ç½® ---
BASE_URL = "http://localhost:8888/v1"
API_KEY = "your_password"  # æ›¿æ¢ä¸ºä½ çš„å¯†ç 
TEST_MODELS = [
    "gemini-2.5-flash",
    "gemini-2.5-pro",
    "gemini-2.5-pro-maxthinking",
]

# è®°å½•æµ‹è¯•å‚æ•°
logging.info("=" * 60)
logging.info("OpenAI API æµ‹è¯•å¼€å§‹")
logging.info(f"API Base URL: {BASE_URL}")
logging.info(f"æµ‹è¯•æ¨¡å‹: {TEST_MODELS}")
logging.info("=" * 60)

#%%
def test_list_models():
    """æµ‹è¯•åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    logging.info("\nã€æµ‹è¯•1ã€‘åˆ—å‡ºå¯ç”¨æ¨¡å‹")
    try:
        client = OpenAI(base_url=BASE_URL, api_key=API_KEY)
        models = client.models.list()
        
        logging.info(f"âœ“ æˆåŠŸè·å–æ¨¡å‹åˆ—è¡¨ï¼Œå…± {len(models.data)} ä¸ªæ¨¡å‹")
        
        # åˆ—å‡ºå‰5ä¸ªæ¨¡å‹
        for i, model in enumerate(models.data[:5], 1):
            logging.info(f"  {i}. {model.id}")
        
        if len(models.data) > 5:
            logging.info(f"  ... è¿˜æœ‰ {len(models.data) - 5} ä¸ªæ¨¡å‹")
        
        return True
    except Exception as e:
        logging.error(f"âœ— æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

#%%
def test_chat_completion(model, stream=False):
    """æµ‹è¯•èŠå¤©è¡¥å…¨"""
    mode_text = "æµå¼" if stream else "éæµå¼"
    logging.info(f"\nã€æµ‹è¯•2ã€‘{mode_text}èŠå¤©è¡¥å…¨ - æ¨¡å‹: {model}")
    
    try:
        client = OpenAI(base_url=BASE_URL, api_key=API_KEY)
        
        messages = [
            {"role": "user", "content": "ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯é‡å­çº ç¼ "}
        ]
        
        logging.info(f"å‘é€æ¶ˆæ¯: {messages[0]['content']}")
        
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            stream=stream,
            max_tokens=100
        )
        
        if stream:
            logging.info("æ¥æ”¶å“åº”ï¼ˆæµå¼ï¼‰:")
            full_response = ""
            for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
            
            logging.info(f"å®Œæ•´å“åº”: {full_response}")
        else:
            content = response.choices[0].message.content
            logging.info(f"å“åº”å†…å®¹: {content}")
        
        logging.info(f"âœ“ {mode_text}æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        logging.error(f"âœ— æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

#%%
def test_thinking_model():
    """æµ‹è¯•å¸¦æ€ç»´è¿‡ç¨‹çš„æ¨¡å‹"""
    logging.info(f"\nã€æµ‹è¯•3ã€‘æ€ç»´è¿‡ç¨‹æµ‹è¯• - æ¨¡å‹: gemini-2.5-pro-maxthinking")
    
    try:
        client = OpenAI(base_url=BASE_URL, api_key=API_KEY)
        
        messages = [
            {"role": "user", "content": "è®¡ç®—ï¼š23 Ã— 47 = ?"}
        ]
        
        logging.info(f"å‘é€æ¶ˆæ¯: {messages[0]['content']}")
        
        response = client.chat.completions.create(
            model="gemini-2.5-pro-maxthinking",
            messages=messages,
            stream=True
        )
        
        logging.info("æ¥æ”¶å“åº”:")
        has_reasoning = False
        reasoning_parts = []
        content_parts = []
        
        for chunk in response:
            if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                has_reasoning = True
                reasoning_parts.append(chunk.choices[0].delta.reasoning_content)
            
            if chunk.choices[0].delta.content:
                content_parts.append(chunk.choices[0].delta.content)
        
        if has_reasoning:
            logging.info(f"[æ¨ç†è¿‡ç¨‹]: {''.join(reasoning_parts)}")
        
        logging.info(f"[æœ€ç»ˆç­”æ¡ˆ]: {''.join(content_parts)}")
        
        if has_reasoning:
            logging.info("âœ“ æ€ç»´è¿‡ç¨‹æµ‹è¯•æˆåŠŸï¼ˆåŒ…å«æ¨ç†è¿‡ç¨‹ï¼‰")
        else:
            logging.info("âœ“ æµ‹è¯•æˆåŠŸï¼ˆä½†æœªæ£€æµ‹åˆ°æ¨ç†è¿‡ç¨‹ï¼‰")
        
        return True
        
    except Exception as e:
        logging.error(f"âœ— æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

#%%
def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logging.info("\n" + "=" * 60)
    logging.info("å¼€å§‹è¿è¡Œæ‰€æœ‰æµ‹è¯•")
    logging.info("=" * 60)
    
    results = {}
    
    # æµ‹è¯•1ï¼šåˆ—å‡ºæ¨¡å‹
    results['list_models'] = test_list_models()
    
    # æµ‹è¯•2ï¼šéæµå¼èŠå¤©ï¼ˆä½¿ç”¨Flashæ¨¡å‹é€Ÿåº¦å¿«ï¼‰
    results['non_stream'] = test_chat_completion("gemini-2.5-flash", stream=False)
    
    # æµ‹è¯•3ï¼šæµå¼èŠå¤©
    results['stream'] = test_chat_completion("gemini-2.5-flash", stream=True)
    
    # æµ‹è¯•4ï¼šæ€ç»´è¿‡ç¨‹
    results['thinking'] = test_thinking_model()
    
    # æ€»ç»“
    logging.info("\n" + "=" * 60)
    logging.info("æµ‹è¯•ç»“æœæ±‡æ€»")
    logging.info("=" * 60)
    
    for test_name, success in results.items():
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        logging.info(f"{test_name}: {status}")
    
    total = len(results)
    passed = sum(results.values())
    logging.info(f"\næ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        logging.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        logging.warning(f"âš ï¸ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
    
    logging.info("=" * 60)

#%%
if __name__ == "__main__":
    run_all_tests()

